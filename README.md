# Content Moderation Benchmark

A comprehensive framework for benchmarking content moderation API providers. Compare performance, accuracy, and reliability across different vendors like Shumei (æ•°ç¾), Huoshan/Volcengine (ç«å±±å¼•æ“), NetEase Yidun (ç½‘æ˜“æ˜“ç›¾), Juntong (å›åŒ), and more.

## Features

- ğŸš€ **Multi-provider Support**: Easily switch between and compare different content moderation providers
- ğŸ“Š **Comprehensive Metrics**: Track response time, QPS, accuracy, precision, recall, and F1 score
- ğŸ“ˆ **Performance Benchmarking**: Concurrent testing with configurable parallelism
- ğŸ“ **Multiple Data Formats**: Support for Excel, JSON, and CSV test data
- ğŸ“‹ **Detailed Reports**: Generate Markdown and JSON reports
- ğŸ”„ **Provider Comparison**: Side-by-side comparison of multiple providers
- â° **Scheduled Benchmarking**: Support for automated scheduled benchmark runs

## Supported Providers

| Provider | Module | Status | Text | Image |
|----------|--------|--------|------|-------|
| æ•°ç¾ç§‘æŠ€ (Shumei) | `shumei` | âœ… Ready | âœ… | âœ… |
| ç«å±±å¼•æ“ (Huoshan/Volcengine) | `huoshan` | âœ… Ready | âœ… | âœ… (BASE64) |
| ç½‘æ˜“æ˜“ç›¾ (NetEase Yidun) | `yidun` | âœ… Ready | âœ… | âœ… |
| å›åŒæœªæ¥ (Juntong) | `juntong` | âœ… Ready | âœ… | âœ… |

## Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/content-moderation-benchmark.git
cd content-moderation-benchmark

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your API credentials
```

Configure your provider credentials in `.env`:

```env
# Shumei (æ•°ç¾ç§‘æŠ€)
SHUMEI_ACCESS_KEY=your_access_key_here
SHUMEI_APP_ID=default
```

### 3. Run Benchmark

```bash
# Initialize project directories
python main.py init

# Run a quick connectivity test
python main.py quick-test -p shumei

# Run full benchmark
python main.py run -p shumei -d your_test_data.xlsx -l 100
```

## Usage

### Run Benchmark for Single Provider

```bash
python main.py run --provider shumei --data test_data.xlsx --limit 100

# Options:
#   -p, --provider    Provider name (required)
#   -d, --data        Path to test data file (required)
#   -l, --limit       Limit number of test cases
#   --text/--no-text  Enable/disable text tests (default: enabled)
#   --image/--no-image Enable/disable image tests (default: enabled)
#   -o, --output      Output report filename
#   -f, --format      Output format: md, json, or both (default: both)
```

### Compare Multiple Providers

```bash
python main.py compare --providers shumei,yidun --data test_data.xlsx -l 500
```

### Quick Connectivity Test

```bash
python main.py quick-test --provider shumei --samples 10
```

### List Available Providers

```bash
python main.py list-providers
```

### Create Sample Test Data

```bash
python main.py create-sample --output sample_data.json --format json
```

## Test Data Format

### Excel Format

Create an Excel file with sheets named "æ–‡æœ¬æµ‹è¯•é¢˜" and "å›¾ç‰‡æµ‹è¯•é¢˜":

| ç±»å‹ | åºå· | å†…å®¹ | é¢„æœŸé£é™© |
|------|------|------|----------|
| é»‘æ ·æœ¬ | 1 | æ•æ„Ÿå†…å®¹... | æ¶‰æ”¿ |
| ç™½æ ·æœ¬ | 2 | æ­£å¸¸å†…å®¹... | æ­£å¸¸ |

### JSON Format

```json
{
  "text": [
    {"id": "text_001", "content": "æµ‹è¯•æ–‡æœ¬", "expected_risk": "æ­£å¸¸", "category": "ç™½æ ·æœ¬"},
    {"id": "text_002", "content": "æ•æ„Ÿå†…å®¹", "expected_risk": "æ¶‰æ”¿", "category": "é»‘æ ·æœ¬"}
  ],
  "image": [
    {"id": "img_001", "content": "https://example.com/image.jpg", "expected_risk": "æ­£å¸¸"}
  ]
}
```

### CSV Format

```csv
id,content,expected_risk,category
text_001,æµ‹è¯•æ–‡æœ¬,æ­£å¸¸,ç™½æ ·æœ¬
text_002,æ•æ„Ÿå†…å®¹,æ¶‰æ”¿,é»‘æ ·æœ¬
```

## Metrics Explained

### Performance Metrics

| Metric | Description |
|--------|-------------|
| Avg Response Time | Average API response time |
| P50/P95/P99 | Response time percentiles |
| QPS | Queries per second |
| Success Rate | Percentage of successful API calls |

### Accuracy Metrics

| Metric | Description |
|--------|-------------|
| Accuracy | Overall correct predictions |
| Precision | True positives / (True positives + False positives) |
| Recall | True positives / (True positives + False negatives) |
| F1 Score | Harmonic mean of precision and recall |

## Project Structure

```
content-moderation-benchmark/
â”œâ”€â”€ main.py                  # CLIä¸»å…¥å£
â”œâ”€â”€ scheduled_benchmark.py   # å®šæ—¶åŸºå‡†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ requirements.txt         # Pythonä¾èµ–
â”œâ”€â”€ .env.example            # ç¯å¢ƒå˜é‡æ¨¡æ¿
â”œâ”€â”€ .gitignore              # Gitå¿½ç•¥è§„åˆ™
â”œâ”€â”€ README.md               # æœ¬æ–‡æ¡£
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # é…ç½®ç®¡ç†
â”‚   â”‚
â”‚   â”œâ”€â”€ providers/          # æœåŠ¡å•†å®ç°
â”‚   â”‚   â”œâ”€â”€ __init__.py     # Provideræ³¨å†Œ
â”‚   â”‚   â”œâ”€â”€ base.py         # æŠ½è±¡åŸºç±»
â”‚   â”‚   â”œâ”€â”€ shumei.py       # æ•°ç¾ç§‘æŠ€
â”‚   â”‚   â”œâ”€â”€ huoshan.py      # ç«å±±å¼•æ“ (LLM Shield)
â”‚   â”‚   â”œâ”€â”€ yidun.py        # ç½‘æ˜“æ˜“ç›¾
â”‚   â”‚   â””â”€â”€ juntong.py      # å›åŒæœªæ¥
â”‚   â”‚
â”‚   â”œâ”€â”€ data/               # æ•°æ®åŠ è½½
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py       # å¤šæ ¼å¼æ•°æ®åŠ è½½å™¨
â”‚   â”‚   â””â”€â”€ datasets.py     # æ•°æ®é›†ç®¡ç†
â”‚   â”‚
â”‚   â””â”€â”€ benchmark/          # åŸºå‡†æµ‹è¯•æ‰§è¡Œ
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ runner.py       # æµ‹è¯•è¿è¡Œå™¨
â”‚       â”œâ”€â”€ metrics.py      # æŒ‡æ ‡æ”¶é›†
â”‚       â”œâ”€â”€ reporter.py     # æŠ¥å‘Šç”Ÿæˆ
â”‚       â””â”€â”€ utils.py        # å·¥å…·å‡½æ•°
â”‚
â”œâ”€â”€ data/                   # æµ‹è¯•æ•°æ®ç›®å½•
â”œâ”€â”€ docs/                   # æ–‡æ¡£ç›®å½•
â”‚   â””â”€â”€ EC2_DEPLOYMENT.md   # EC2éƒ¨ç½²æŒ‡å—
â”œâ”€â”€ output/                 # æµ‹è¯•è¾“å‡º (gitignored)
â””â”€â”€ reports/                # ç”Ÿæˆçš„æŠ¥å‘Š (gitignored)
```

## Adding a New Provider

1. Create a new file in `src/providers/`:

```python
# src/providers/myprovider.py
import os
from .base import BaseProvider, ModerationResult, ContentType, RiskLevel, ConfigurationError
from ..config import Config

class MyProvider(BaseProvider):
    name = "myprovider"
    display_name = "My Provider"
    
    def _load_config(self):
        return {
            "api_key": os.getenv("MYPROVIDER_API_KEY"),
            # ... other config
        }
    
    def _validate_config(self) -> None:
        if not self.config.get("api_key"):
            raise ConfigurationError("MYPROVIDER_API_KEY is required.")
    
    def moderate_text(self, text: str, **kwargs) -> ModerationResult:
        # Implement API call
        result = ModerationResult(provider=self.name, content_type=ContentType.TEXT)
        # ... call API and parse response
        return result
    
    def moderate_image(self, image_url: str, **kwargs) -> ModerationResult:
        # Implement API call
        result = ModerationResult(provider=self.name, content_type=ContentType.IMAGE)
        # ... call API and parse response
        return result
```

2. Add configuration loader in `src/config.py`:

```python
@staticmethod
def get_myprovider_config() -> Dict[str, Any]:
    return {
        "api_key": os.getenv("MYPROVIDER_API_KEY"),
        # ... other config
    }
```

3. Register in `src/providers/__init__.py`:

```python
from .myprovider import MyProvider

PROVIDERS = {
    "shumei": ShumeiProvider,
    "huoshan": HuoshanProvider,
    "yidun": YidunProvider,
    "juntong": JunTongProvider,
    "myprovider": MyProvider,  # Add this line
}
```

4. Add configuration to `.env.example`:

```env
# My Provider
MYPROVIDER_API_KEY=your_api_key_here
```

## Sample Output

### Console Output

```
Content Moderation Benchmark
Provider: shumei
Data: test_data.xlsx
Limit: 100

âœ… Provider initialized: æ•°ç¾ç§‘æŠ€

============================================================
ğŸ“Š Benchmark Summary: shumei
============================================================

ğŸ“ Text Moderation:
   Total: 100 requests
   Success Rate: 100.0%
   Avg Response: 85ms
   P99 Response: 156ms
   Accuracy: 94.0%
   Recall: 92.5%
   F1 Score: 93.2

============================================================
```

### Markdown Report

Reports are generated in the `reports/` directory with detailed metrics, confusion matrices, and recommendations.

## Environment Variables

### Shumei (æ•°ç¾ç§‘æŠ€)

| Variable | Description | Required |
|----------|-------------|----------|
| `SHUMEI_ACCESS_KEY` | æ•°ç¾API Access Key | Yes |
| `SHUMEI_APP_ID` | åº”ç”¨ID | No (default: 'default') |
| `SHUMEI_TEXT_URL` | æ–‡æœ¬å®¡æ ¸APIåœ°å€ | No (has default) |
| `SHUMEI_IMAGE_URL` | å›¾ç‰‡å®¡æ ¸APIåœ°å€ | No (has default) |

### Huoshan/Volcengine (ç«å±±å¼•æ“)

| Variable | Description | Required |
|----------|-------------|----------|
| `HUOSHAN_ACCESS_KEY` | ç«å±±å¼•æ“ Access Key | Yes |
| `HUOSHAN_SECRET_KEY` | ç«å±±å¼•æ“ Secret Key | Yes |
| `HUOSHAN_APP_ID` | LLM Shield AppID | Yes |
| `HUOSHAN_REGION` | åŒºåŸŸ (cn-beijing/cn-shanghai) | No (default: cn-beijing) |
| `HUOSHAN_CUSTOM_URL` | è‡ªå®šä¹‰API URL | No |

### NetEase Yidun (ç½‘æ˜“æ˜“ç›¾)

| Variable | Description | Required |
|----------|-------------|----------|
| `YIDUN_SECRET_ID` | æ˜“ç›¾ Secret ID | Yes |
| `YIDUN_SECRET_KEY` | æ˜“ç›¾ Secret Key | Yes |
| `YIDUN_BUSINESS_ID_TEXT` | æ–‡æœ¬å®¡æ ¸ä¸šåŠ¡ID | Yes (for text) |
| `YIDUN_BUSINESS_ID_IMAGE` | å›¾ç‰‡å®¡æ ¸ä¸šåŠ¡ID | Yes (for image) |

### Juntong (å›åŒæœªæ¥)

| Variable | Description | Required |
|----------|-------------|----------|
| `JUNTONG_TEXT_API_KEY` | æ–‡æœ¬å®¡æ ¸API Key | Yes (for text) |
| `JUNTONG_IMAGE_API_KEY` | å›¾ç‰‡å®¡æ ¸API Key | Yes (for image) |
| `JUNTONG_BASE_URL` | APIåŸºç¡€URL | No (has default) |

### Benchmark Configuration

| Variable | Description | Required |
|----------|-------------|----------|
| `MAX_WORKERS` | å¹¶å‘å·¥ä½œçº¿ç¨‹æ•° | No (default: 10) |
| `REQUEST_INTERVAL` | è¯·æ±‚é—´éš”(ç§’) | No (default: 0.1) |
| `REQUEST_TIMEOUT` | è¯·æ±‚è¶…æ—¶(ç§’) | No (default: 30) |
| `RETRY_TIMES` | é‡è¯•æ¬¡æ•° | No (default: 3) |

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-provider`)
3. Commit your changes (`git commit -am 'Add new provider'`)
4. Push to the branch (`git push origin feature/new-provider`)
5. Create a Pull Request

## License

MIT License - see LICENSE file for details.

## Acknowledgments

- Built for comparing content moderation services in production environments
- Inspired by the need for standardized benchmarking across providers
