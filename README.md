# Content Moderation Benchmark

A comprehensive framework for benchmarking content moderation API providers. Compare performance, accuracy, and reliability across different vendors like Shumei (Êï∞Áæé), Bytedance (Â≠óËäÇ), NetEase Yidun (ÁΩëÊòìÊòìÁõæ), Juntong (ÂêõÂêå), and more.

## Features

- üöÄ **Multi-provider Support**: Easily switch between and compare different content moderation providers
- üìä **Comprehensive Metrics**: Track response time, QPS, accuracy, precision, recall, and F1 score
- üìà **Performance Benchmarking**: Concurrent testing with configurable parallelism
- üìù **Multiple Data Formats**: Support for Excel, JSON, and CSV test data
- üìã **Detailed Reports**: Generate Markdown and JSON reports
- üîÑ **Provider Comparison**: Side-by-side comparison of multiple providers

## Supported Providers

| Provider | Status | Text | Image |
|----------|--------|------|-------|
| Shumei (Êï∞ÁæéÁßëÊäÄ) | ‚úÖ Ready | ‚úÖ | ‚úÖ |
| Bytedance (Â≠óËäÇË∑≥Âä®) | üöß Template | - | - |
| NetEase Yidun (ÁΩëÊòìÊòìÁõæ) | üöß Template | - | - |
| Juntong (ÂêõÂêå) | üöß Template | - | - |

## Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/content-moderation-benchmark.git
cd content-moderation-benchmark

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

```bash
# Copy the example environment file
cp .env.example .env

# Edit .env with your API credentials
```

Configure your provider credentials in `.env`:

```env
# Shumei (Êï∞ÁæéÁßëÊäÄ)
SHUMEI_ACCESS_KEY=your_access_key_here
SHUMEI_APP_ID=default
```

### 3. Run Benchmark

```bash
# Initialize project directories
python main.py init

# Run a quick connectivity test
python main.py quick-test -p shumei

# Run full benchmark
python main.py run -p shumei -d your_test_data.xlsx -l 100
```

## Usage

### Run Benchmark for Single Provider

```bash
python main.py run --provider shumei --data test_data.xlsx --limit 100

# Options:
#   -p, --provider    Provider name (required)
#   -d, --data        Path to test data file (required)
#   -l, --limit       Limit number of test cases
#   --text/--no-text  Enable/disable text tests (default: enabled)
#   --image/--no-image Enable/disable image tests (default: enabled)
#   -o, --output      Output report filename
#   -f, --format      Output format: md, json, or both (default: both)
```

### Compare Multiple Providers

```bash
python main.py compare --providers shumei,yidun --data test_data.xlsx -l 500
```

### Quick Connectivity Test

```bash
python main.py quick-test --provider shumei --samples 10
```

### List Available Providers

```bash
python main.py list-providers
```

### Create Sample Test Data

```bash
python main.py create-sample --output sample_data.json --format json
```

## Test Data Format

### Excel Format

Create an Excel file with sheets named "ÊñáÊú¨ÊµãËØïÈ¢ò" and "ÂõæÁâáÊµãËØïÈ¢ò":

| Á±ªÂûã | Â∫èÂè∑ | ÂÜÖÂÆπ | È¢ÑÊúüÈ£éÈô© |
|------|------|------|----------|
| ÈªëÊ†∑Êú¨ | 1 | ÊïèÊÑüÂÜÖÂÆπ... | Ê∂âÊîø |
| ÁôΩÊ†∑Êú¨ | 2 | Ê≠£Â∏∏ÂÜÖÂÆπ... | Ê≠£Â∏∏ |

### JSON Format

```json
{
  "text": [
    {"id": "text_001", "content": "ÊµãËØïÊñáÊú¨", "expected_risk": "Ê≠£Â∏∏", "category": "ÁôΩÊ†∑Êú¨"},
    {"id": "text_002", "content": "ÊïèÊÑüÂÜÖÂÆπ", "expected_risk": "Ê∂âÊîø", "category": "ÈªëÊ†∑Êú¨"}
  ],
  "image": [
    {"id": "img_001", "content": "https://example.com/image.jpg", "expected_risk": "Ê≠£Â∏∏"}
  ]
}
```

### CSV Format

```csv
id,content,expected_risk,category
text_001,ÊµãËØïÊñáÊú¨,Ê≠£Â∏∏,ÁôΩÊ†∑Êú¨
text_002,ÊïèÊÑüÂÜÖÂÆπ,Ê∂âÊîø,ÈªëÊ†∑Êú¨
```

## Metrics Explained

### Performance Metrics

| Metric | Description |
|--------|-------------|
| Avg Response Time | Average API response time |
| P50/P95/P99 | Response time percentiles |
| QPS | Queries per second |
| Success Rate | Percentage of successful API calls |

### Accuracy Metrics

| Metric | Description |
|--------|-------------|
| Accuracy | Overall correct predictions |
| Precision | True positives / (True positives + False positives) |
| Recall | True positives / (True positives + False negatives) |
| F1 Score | Harmonic mean of precision and recall |

## Project Structure

```
content-moderation-benchmark/
‚îú‚îÄ‚îÄ main.py                 # CLI entry point
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ .env.example           # Environment template
‚îú‚îÄ‚îÄ .gitignore             # Git ignore rules
‚îú‚îÄ‚îÄ README.md              # This file
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Configuration management
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ providers/         # Provider implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py        # Abstract base class
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ shumei.py      # Shumei provider
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data/              # Data loading
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loader.py      # Multi-format data loader
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ benchmark/         # Benchmark execution
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ runner.py      # Benchmark runner
‚îÇ       ‚îú‚îÄ‚îÄ metrics.py     # Metrics collection
‚îÇ       ‚îî‚îÄ‚îÄ reporter.py    # Report generation
‚îÇ
‚îú‚îÄ‚îÄ output/                # Test outputs (gitignored)
‚îî‚îÄ‚îÄ reports/               # Generated reports (gitignored)
```

## Adding a New Provider

1. Create a new file in `src/providers/`:

```python
# src/providers/myprovider.py
from .base import BaseProvider, ModerationResult, ContentType
from ..config import Config

class MyProvider(BaseProvider):
    name = "myprovider"
    display_name = "My Provider"
    
    def _load_config(self):
        return {
            "api_key": os.getenv("MYPROVIDER_API_KEY"),
            # ... other config
        }
    
    def moderate_text(self, text: str, **kwargs) -> ModerationResult:
        # Implement API call
        pass
    
    def moderate_image(self, image_url: str, **kwargs) -> ModerationResult:
        # Implement API call
        pass
```

2. Register in `src/providers/__init__.py`:

```python
from .myprovider import MyProvider

PROVIDERS = {
    "shumei": ShumeiProvider,
    "myprovider": MyProvider,  # Add this line
}
```

3. Add configuration to `.env.example`:

```env
# My Provider
MYPROVIDER_API_KEY=your_api_key_here
```

## Sample Output

### Console Output

```
Content Moderation Benchmark
Provider: shumei
Data: test_data.xlsx
Limit: 100

‚úÖ Provider initialized: Êï∞ÁæéÁßëÊäÄ

============================================================
üìä Benchmark Summary: shumei
============================================================

üìù Text Moderation:
   Total: 100 requests
   Success Rate: 100.0%
   Avg Response: 85ms
   P99 Response: 156ms
   Accuracy: 94.0%
   Recall: 92.5%
   F1 Score: 93.2

============================================================
```

### Markdown Report

Reports are generated in the `reports/` directory with detailed metrics, confusion matrices, and recommendations.

## Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `SHUMEI_ACCESS_KEY` | Shumei API access key | Yes (for Shumei) |
| `SHUMEI_APP_ID` | Shumei application ID | No (default: 'default') |
| `MAX_WORKERS` | Concurrent workers | No (default: 10) |
| `REQUEST_INTERVAL` | Delay between requests (seconds) | No (default: 0.1) |
| `REQUEST_TIMEOUT` | Request timeout (seconds) | No (default: 30) |

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-provider`)
3. Commit your changes (`git commit -am 'Add new provider'`)
4. Push to the branch (`git push origin feature/new-provider`)
5. Create a Pull Request

## License

MIT License - see LICENSE file for details.

## Acknowledgments

- Built for comparing content moderation services in production environments
- Inspired by the need for standardized benchmarking across providers
