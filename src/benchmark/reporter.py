"""
Report generation for benchmark results.
Supports Markdown and JSON output formats.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from .metrics import BenchmarkMetrics
from .runner import BenchmarkResult
from .utils import get_machine_info, get_report_subdir_name
from ..config import Config


class Reporter:
    """
    Generate benchmark reports in various formats.
    
    Supports:
        - Markdown reports
        - JSON data export
        - Console output
        - Multi-provider comparison reports
    
    Reports are organized by date, region and IP address:
        reports/YYYYMMDD_region_ip/
    
    Example:
        reporter = Reporter()
        reporter.generate_markdown(result, "report.md")
        reporter.generate_json(result, "results.json")
    """
    
    def __init__(self, output_dir: Optional[Path] = None):
        """
        Initialize reporter.
        
        Args:
            output_dir: Base directory for output files (default: Config.REPORT_DIR)
        """
        base_dir = output_dir or Config.REPORT_DIR
        
        # Create subdirectory with date_region_ip format
        subdir_name = get_report_subdir_name()
        self.output_dir = base_dir / subdir_name
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Cache machine info for this reporter instance
        self._machine_info = get_machine_info()
    
    def generate_markdown(
        self,
        result: BenchmarkResult,
        filename: Optional[str] = None,
    ) -> str:
        """
        Generate a Markdown benchmark report.
        
        Args:
            result: Benchmark result to report
            filename: Output filename (optional)
            
        Returns:
            Path to generated file
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if not filename:
            filename = f"benchmark_report_{result.provider}_{file_timestamp}.md"
        
        output_path = self.output_dir / filename
        
        # Use cached machine info
        machine_info = self._machine_info
        
        lines = []
        lines.append(f"# å†…å®¹å®¡æ ¸æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        lines.append(f"\n**ä¾›åº”å•†:** {result.provider}")
        lines.append(f"**ç”Ÿæˆæ—¶é—´:** {timestamp}")
        lines.append(f"\n---\n")
        
        # Machine info section
        lines.append("## æµ‹è¯•ç¯å¢ƒ\n")
        lines.append("| é¡¹ç›® | ä¿¡æ¯ |")
        lines.append("|------|------|")
        lines.append(f"| åŒºåŸŸ (Region) | {machine_info['region']} |")
        lines.append(f"| å¯ç”¨åŒº (AZ) | {machine_info['availability_zone']} |")
        lines.append(f"| å®ä¾‹ID | {machine_info['instance_id']} |")
        lines.append(f"| ä¸»æœºå | {machine_info['hostname']} |")
        lines.append(f"| IPåœ°å€ | {machine_info['ip_address']} |")
        lines.append(f"| æ“ä½œç³»ç»Ÿ | {machine_info['platform']} |")
        lines.append(f"\n---\n")
        
        # Text metrics
        if result.text_metrics:
            lines.append(self._format_metrics_section(
                "æ–‡æœ¬å®¡æ ¸", 
                result.text_metrics,
            ))
        
        # Image metrics
        if result.image_metrics:
            lines.append(self._format_metrics_section(
                "å›¾ç‰‡å®¡æ ¸",
                result.image_metrics,
            ))
        
        # Summary
        lines.append("\n## æ€»ç»“\n")
        lines.append(self._generate_summary(result))
        
        content = "\n".join(lines)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        return str(output_path)
    
    def _format_metrics_section(
        self, 
        title: str, 
        metrics: BenchmarkMetrics,
    ) -> str:
        """Format a metrics section for Markdown."""
        lines = []
        lines.append(f"\n## {title}\n")
        
        # Overview table
        lines.append("### æ¦‚è§ˆ\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| è¯·æ±‚æ€»æ•° | {metrics.total_requests} |")
        lines.append(f"| æˆåŠŸæ•° | {metrics.success_count} |")
        lines.append(f"| å¤±è´¥æ•° | {metrics.fail_count} |")
        lines.append(f"| æˆåŠŸç‡ | {metrics.success_rate:.2f}% |")
        
        # Performance table
        lines.append("\n### æ€§èƒ½æŒ‡æ ‡\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| å¹³å‡å“åº”æ—¶é—´ | {metrics.avg_response_time*1000:.0f}ms |")
        lines.append(f"| P50å“åº”æ—¶é—´ | {metrics.p50_response_time*1000:.0f}ms |")
        lines.append(f"| P95å“åº”æ—¶é—´ | {metrics.p95_response_time*1000:.0f}ms |")
        lines.append(f"| P99å“åº”æ—¶é—´ | {metrics.p99_response_time*1000:.0f}ms |")
        lines.append(f"| æœ€å°å“åº”æ—¶é—´ | {metrics.min_response_time*1000:.0f}ms |")
        lines.append(f"| æœ€å¤§å“åº”æ—¶é—´ | {metrics.max_response_time*1000:.0f}ms |")
        lines.append(f"| QPS (æ¯ç§’æŸ¥è¯¢æ•°) | {metrics.qps:.2f} |")
        lines.append(f"| æ€»è€—æ—¶ | {metrics.total_duration:.1f}ç§’ |")
        
        # Accuracy table
        lines.append("\n### å‡†ç¡®æ€§æŒ‡æ ‡\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| å‡†ç¡®ç‡ | {metrics.accuracy:.2f}% |")
        lines.append(f"| ç²¾ç¡®ç‡ | {metrics.precision:.2f}% |")
        lines.append(f"| å¬å›ç‡ | {metrics.recall:.2f}% |")
        lines.append(f"| F1åˆ†æ•° | {metrics.f1_score:.2f} |")
        
        # Confusion matrix
        lines.append("\n### æ··æ·†çŸ©é˜µ\n")
        lines.append("| | é¢„æµ‹ä¸ºè¿è§„ | é¢„æµ‹ä¸ºæ­£å¸¸ |")
        lines.append("|---|---|---|")
        lines.append(f"| **å®é™…è¿è§„** | TP: {metrics.true_positive} | FN: {metrics.false_negative} |")
        lines.append(f"| **å®é™…æ­£å¸¸** | FP: {metrics.false_positive} | TN: {metrics.true_negative} |")
        
        # Errors
        if metrics.error_types:
            lines.append("\n### é”™è¯¯ç»Ÿè®¡\n")
            lines.append("| é”™è¯¯ç±»å‹ | æ•°é‡ |")
            lines.append("|----------|------|")
            for error_type, count in metrics.error_types.items():
                lines.append(f"| {error_type} | {count} |")
        
        return "\n".join(lines)
    
    def _generate_summary(self, result: BenchmarkResult) -> str:
        """Generate summary section."""
        lines = []
        
        if result.text_metrics:
            tm = result.text_metrics
            lines.append(f"**æ–‡æœ¬å®¡æ ¸:**")
            lines.append(f"- å¤„ç† {tm.total_requests} ä¸ªè¯·æ±‚ï¼ŒæˆåŠŸç‡ {tm.success_rate:.1f}%")
            lines.append(f"- å¹³å‡å“åº”æ—¶é—´: {tm.avg_response_time*1000:.0f}msï¼ŒP99: {tm.p99_response_time*1000:.0f}ms")
            lines.append(f"- å‡†ç¡®ç‡: {tm.accuracy:.1f}%ï¼Œå¬å›ç‡: {tm.recall:.1f}%ï¼ŒF1: {tm.f1_score:.1f}")
            lines.append("")
        
        if result.image_metrics:
            im = result.image_metrics
            lines.append(f"**å›¾ç‰‡å®¡æ ¸:**")
            lines.append(f"- å¤„ç† {im.total_requests} ä¸ªè¯·æ±‚ï¼ŒæˆåŠŸç‡ {im.success_rate:.1f}%")
            lines.append(f"- å¹³å‡å“åº”æ—¶é—´: {im.avg_response_time*1000:.0f}msï¼ŒP99: {im.p99_response_time*1000:.0f}ms")
            lines.append(f"- å‡†ç¡®ç‡: {im.accuracy:.1f}%ï¼Œå¬å›ç‡: {im.recall:.1f}%ï¼ŒF1: {im.f1_score:.1f}")
        
        return "\n".join(lines)
    
    def generate_json(
        self,
        result: BenchmarkResult,
        filename: Optional[str] = None,
    ) -> str:
        """
        Generate JSON benchmark results.
        
        Args:
            result: Benchmark result to export
            filename: Output filename (optional)
            
        Returns:
            Path to generated file
        """
        file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if not filename:
            filename = f"benchmark_results_{result.provider}_{file_timestamp}.json"
        
        output_path = self.output_dir / filename
        
        # Use cached machine info
        machine_info = self._machine_info
        
        data = {
            "provider": result.provider,
            "generated_at": datetime.now().isoformat(),
            "test_environment": {
                "region": machine_info["region"],
                "availability_zone": machine_info["availability_zone"],
                "instance_id": machine_info["instance_id"],
                "hostname": machine_info["hostname"],
                "ip_address": machine_info["ip_address"],
                "platform": machine_info["platform"],
            },
            "text_metrics": result.text_metrics.to_dict() if result.text_metrics else None,
            "image_metrics": result.image_metrics.to_dict() if result.image_metrics else None,
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return str(output_path)
    
    def generate_comparison_report(
        self,
        results: Dict[str, BenchmarkResult],
        filename: Optional[str] = None,
    ) -> str:
        """
        Generate a comparison report for multiple providers.
        
        Args:
            results: Dictionary mapping provider name to results
            filename: Output filename (optional)
            
        Returns:
            Path to generated file
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if not filename:
            filename = f"benchmark_comparison_{file_timestamp}.md"
        
        output_path = self.output_dir / filename
        
        lines = []
        lines.append("# å†…å®¹å®¡æ ¸ä¾›åº”å•†å¯¹æ¯”æŠ¥å‘Š")
        lines.append(f"\n**ç”Ÿæˆæ—¶é—´:** {timestamp}")
        lines.append(f"**å¯¹æ¯”ä¾›åº”å•†:** {', '.join(results.keys())}")
        lines.append("\n---\n")
        
        # Text comparison
        text_results = {
            k: v.text_metrics for k, v in results.items() 
            if v.text_metrics
        }
        if text_results:
            lines.append(self._format_comparison_table(
                "æ–‡æœ¬å®¡æ ¸å¯¹æ¯”",
                text_results,
            ))
        
        # Image comparison
        image_results = {
            k: v.image_metrics for k, v in results.items()
            if v.image_metrics
        }
        if image_results:
            lines.append(self._format_comparison_table(
                "å›¾ç‰‡å®¡æ ¸å¯¹æ¯”",
                image_results,
            ))
        
        # Recommendations
        lines.append("\n## å»ºè®®\n")
        lines.append(self._generate_recommendations(results))
        
        content = "\n".join(lines)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        return str(output_path)
    
    def _format_comparison_table(
        self,
        title: str,
        metrics_dict: Dict[str, BenchmarkMetrics],
    ) -> str:
        """Format a comparison table for multiple providers."""
        lines = []
        lines.append(f"\n## {title}\n")
        
        providers = list(metrics_dict.keys())
        
        # Performance comparison
        lines.append("### æ€§èƒ½å¯¹æ¯”\n")
        header = "| æŒ‡æ ‡ |" + "|".join(f" {p} " for p in providers) + "|"
        separator = "|------|" + "|".join("------" for _ in providers) + "|"
        lines.append(header)
        lines.append(separator)
        
        rows = [
            ("å¹³å‡å“åº”(ms)", lambda m: f"{m.avg_response_time*1000:.0f}"),
            ("P99å“åº”(ms)", lambda m: f"{m.p99_response_time*1000:.0f}"),
            ("æˆåŠŸç‡(%)", lambda m: f"{m.success_rate:.1f}"),
            ("QPS", lambda m: f"{m.qps:.1f}"),
        ]
        
        for label, getter in rows:
            row = f"| {label} |"
            for p in providers:
                row += f" {getter(metrics_dict[p])} |"
            lines.append(row)
        
        # Accuracy comparison
        lines.append("\n### å‡†ç¡®æ€§å¯¹æ¯”\n")
        lines.append(header)
        lines.append(separator)
        
        rows = [
            ("å‡†ç¡®ç‡(%)", lambda m: f"{m.accuracy:.1f}"),
            ("ç²¾ç¡®ç‡(%)", lambda m: f"{m.precision:.1f}"),
            ("å¬å›ç‡(%)", lambda m: f"{m.recall:.1f}"),
            ("F1åˆ†æ•°", lambda m: f"{m.f1_score:.1f}"),
        ]
        
        for label, getter in rows:
            row = f"| {label} |"
            for p in providers:
                row += f" {getter(metrics_dict[p])} |"
            lines.append(row)
        
        return "\n".join(lines)
    
    def _generate_recommendations(
        self, 
        results: Dict[str, BenchmarkResult],
    ) -> str:
        """Generate recommendations based on results."""
        lines = []
        
        # Find best performers
        text_results = {
            k: v.text_metrics for k, v in results.items()
            if v.text_metrics
        }
        
        if text_results:
            # Best response time
            best_speed = min(
                text_results.items(),
                key=lambda x: x[1].avg_response_time,
            )
            lines.append(f"- **å“åº”æœ€å¿«:** {best_speed[0]} (å¹³å‡ {best_speed[1].avg_response_time*1000:.0f}ms)")
            
            # Best accuracy
            best_accuracy = max(
                text_results.items(),
                key=lambda x: x[1].accuracy,
            )
            lines.append(f"- **å‡†ç¡®ç‡æœ€é«˜:** {best_accuracy[0]} ({best_accuracy[1].accuracy:.1f}%)")
            
            # Best recall
            best_recall = max(
                text_results.items(),
                key=lambda x: x[1].recall,
            )
            lines.append(f"- **å¬å›ç‡æœ€é«˜:** {best_recall[0]} ({best_recall[1].recall:.1f}%)")
        
        lines.append("\n### åœºæ™¯æ¨è\n")
        lines.append("| ä½¿ç”¨åœºæ™¯ | æ¨èä¾›åº”å•† | åŸå›  |")
        lines.append("|----------|------------|------|")
        
        if text_results:
            best_speed_name = min(text_results.items(), key=lambda x: x[1].avg_response_time)[0]
            best_recall_name = max(text_results.items(), key=lambda x: x[1].recall)[0]
            best_f1_name = max(text_results.items(), key=lambda x: x[1].f1_score)[0]
            
            lines.append(f"| å®æ—¶å®¡æ ¸ | {best_speed_name} | å“åº”é€Ÿåº¦æœ€å¿« |")
            lines.append(f"| é«˜é£é™©å†…å®¹ | {best_recall_name} | å¬å›ç‡æœ€é«˜ï¼Œæ¼æ£€æœ€å°‘ |")
            lines.append(f"| ç»¼åˆå¹³è¡¡ | {best_f1_name} | F1åˆ†æ•°æœ€ä¼˜ |")
        
        return "\n".join(lines)
    
    def print_summary(self, result: BenchmarkResult) -> None:
        """Print a summary to console."""
        print("\n" + "="*60)
        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦: {result.provider}")
        print("="*60)
        
        if result.text_metrics:
            tm = result.text_metrics
            print(f"\nğŸ“ æ–‡æœ¬å®¡æ ¸:")
            print(f"   è¯·æ±‚æ€»æ•°: {tm.total_requests}")
            print(f"   æˆåŠŸç‡: {tm.success_rate:.1f}%")
            print(f"   å¹³å‡å“åº”: {tm.avg_response_time*1000:.0f}ms")
            print(f"   P99å“åº”: {tm.p99_response_time*1000:.0f}ms")
            print(f"   å‡†ç¡®ç‡: {tm.accuracy:.1f}%")
            print(f"   å¬å›ç‡: {tm.recall:.1f}%")
            print(f"   F1åˆ†æ•°: {tm.f1_score:.1f}")
        
        if result.image_metrics:
            im = result.image_metrics
            print(f"\nğŸ–¼ï¸  å›¾ç‰‡å®¡æ ¸:")
            print(f"   è¯·æ±‚æ€»æ•°: {im.total_requests}")
            print(f"   æˆåŠŸç‡: {im.success_rate:.1f}%")
            print(f"   å¹³å‡å“åº”: {im.avg_response_time*1000:.0f}ms")
            print(f"   P99å“åº”: {im.p99_response_time*1000:.0f}ms")
            print(f"   å‡†ç¡®ç‡: {im.accuracy:.1f}%")
            print(f"   å¬å›ç‡: {im.recall:.1f}%")
            print(f"   F1åˆ†æ•°: {im.f1_score:.1f}")
        
        print("\n" + "="*60)
