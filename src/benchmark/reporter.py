"""
Report generation for benchmark results.
Supports Markdown and JSON output formats.
"""

import json
import socket
import platform
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

from .metrics import BenchmarkMetrics
from .runner import BenchmarkResult
from ..config import Config


def get_machine_info() -> Dict[str, str]:
    """
    Get machine information for report context.
    
    Returns:
        Dictionary with machine details including:
        - hostname: Machine hostname
        - ip_address: Local IP address
        - region: AWS region (if on EC2) or 'local'
        - instance_id: EC2 instance ID (if on EC2)
        - platform: OS platform info
    """
    info = {
        "hostname": socket.gethostname(),
        "platform": f"{platform.system()} {platform.release()}",
        "region": "unknown",
        "instance_id": "N/A",
        "availability_zone": "N/A",
    }
    
    # Try to get local IP
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        info["ip_address"] = s.getsockname()[0]
        s.close()
    except Exception:
        info["ip_address"] = "127.0.0.1"
    
    # Try to get AWS EC2 metadata (IMDSv2)
    try:
        import urllib.request
        
        # First get token for IMDSv2
        token_request = urllib.request.Request(
            "http://169.254.169.254/latest/api/token",
            method="PUT",
            headers={"X-aws-ec2-metadata-token-ttl-seconds": "21600"}
        )
        token_request.timeout = 1
        
        with urllib.request.urlopen(token_request, timeout=1) as response:
            token = response.read().decode('utf-8')
        
        headers = {"X-aws-ec2-metadata-token": token}
        
        # Get availability zone
        az_request = urllib.request.Request(
            "http://169.254.169.254/latest/meta-data/placement/availability-zone",
            headers=headers
        )
        with urllib.request.urlopen(az_request, timeout=1) as response:
            az = response.read().decode('utf-8')
            info["availability_zone"] = az
            # Region is AZ without the last character (e.g., us-east-1a -> us-east-1)
            info["region"] = az[:-1]
        
        # Get instance ID
        instance_request = urllib.request.Request(
            "http://169.254.169.254/latest/meta-data/instance-id",
            headers=headers
        )
        with urllib.request.urlopen(instance_request, timeout=1) as response:
            info["instance_id"] = response.read().decode('utf-8')
            
    except Exception:
        # Not on AWS EC2 or metadata not available
        # Check for AWS_DEFAULT_REGION or AWS_REGION environment variable
        info["region"] = os.environ.get("AWS_DEFAULT_REGION", 
                                        os.environ.get("AWS_REGION", "local"))
    
    return info


class Reporter:
    """
    Generate benchmark reports in various formats.
    
    Supports:
        - Markdown reports
        - JSON data export
        - Console output
        - Multi-provider comparison reports
    
    Example:
        reporter = Reporter()
        reporter.generate_markdown(result, "report.md")
        reporter.generate_json(result, "results.json")
    """
    
    def __init__(self, output_dir: Optional[Path] = None):
        """
        Initialize reporter.
        
        Args:
            output_dir: Directory for output files
        """
        self.output_dir = output_dir or Config.REPORT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def generate_markdown(
        self,
        result: BenchmarkResult,
        filename: Optional[str] = None,
    ) -> str:
        """
        Generate a Markdown benchmark report.
        
        Args:
            result: Benchmark result to report
            filename: Output filename (optional)
            
        Returns:
            Path to generated file
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if not filename:
            filename = f"benchmark_report_{result.provider}_{file_timestamp}.md"
        
        output_path = self.output_dir / filename
        
        # Get machine info
        machine_info = get_machine_info()
        
        lines = []
        lines.append(f"# å†…å®¹å®¡æ ¸æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        lines.append(f"\n**ä¾›åº”å•†:** {result.provider}")
        lines.append(f"**ç”Ÿæˆæ—¶é—´:** {timestamp}")
        lines.append(f"\n---\n")
        
        # Machine info section
        lines.append("## æµ‹è¯•ç¯å¢ƒ\n")
        lines.append("| é¡¹ç›® | ä¿¡æ¯ |")
        lines.append("|------|------|")
        lines.append(f"| åŒºåŸŸ (Region) | {machine_info['region']} |")
        lines.append(f"| å¯ç”¨åŒº (AZ) | {machine_info['availability_zone']} |")
        lines.append(f"| å®ä¾‹ID | {machine_info['instance_id']} |")
        lines.append(f"| ä¸»æœºå | {machine_info['hostname']} |")
        lines.append(f"| IPåœ°å€ | {machine_info['ip_address']} |")
        lines.append(f"| æ“ä½œç³»ç»Ÿ | {machine_info['platform']} |")
        lines.append(f"\n---\n")
        
        # Text metrics
        if result.text_metrics:
            lines.append(self._format_metrics_section(
                "æ–‡æœ¬å®¡æ ¸", 
                result.text_metrics,
            ))
        
        # Image metrics
        if result.image_metrics:
            lines.append(self._format_metrics_section(
                "å›¾ç‰‡å®¡æ ¸",
                result.image_metrics,
            ))
        
        # Summary
        lines.append("\n## æ€»ç»“\n")
        lines.append(self._generate_summary(result))
        
        content = "\n".join(lines)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        return str(output_path)
    
    def _format_metrics_section(
        self, 
        title: str, 
        metrics: BenchmarkMetrics,
    ) -> str:
        """Format a metrics section for Markdown."""
        lines = []
        lines.append(f"\n## {title}\n")
        
        # Overview table
        lines.append("### æ¦‚è§ˆ\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| è¯·æ±‚æ€»æ•° | {metrics.total_requests} |")
        lines.append(f"| æˆåŠŸæ•° | {metrics.success_count} |")
        lines.append(f"| å¤±è´¥æ•° | {metrics.fail_count} |")
        lines.append(f"| æˆåŠŸç‡ | {metrics.success_rate:.2f}% |")
        
        # Performance table
        lines.append("\n### æ€§èƒ½æŒ‡æ ‡\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| å¹³å‡å“åº”æ—¶é—´ | {metrics.avg_response_time*1000:.0f}ms |")
        lines.append(f"| P50å“åº”æ—¶é—´ | {metrics.p50_response_time*1000:.0f}ms |")
        lines.append(f"| P95å“åº”æ—¶é—´ | {metrics.p95_response_time*1000:.0f}ms |")
        lines.append(f"| P99å“åº”æ—¶é—´ | {metrics.p99_response_time*1000:.0f}ms |")
        lines.append(f"| æœ€å°å“åº”æ—¶é—´ | {metrics.min_response_time*1000:.0f}ms |")
        lines.append(f"| æœ€å¤§å“åº”æ—¶é—´ | {metrics.max_response_time*1000:.0f}ms |")
        lines.append(f"| QPS (æ¯ç§’æŸ¥è¯¢æ•°) | {metrics.qps:.2f} |")
        lines.append(f"| æ€»è€—æ—¶ | {metrics.total_duration:.1f}ç§’ |")
        
        # Accuracy table
        lines.append("\n### å‡†ç¡®æ€§æŒ‡æ ‡\n")
        lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
        lines.append("|------|------|")
        lines.append(f"| å‡†ç¡®ç‡ | {metrics.accuracy:.2f}% |")
        lines.append(f"| ç²¾ç¡®ç‡ | {metrics.precision:.2f}% |")
        lines.append(f"| å¬å›ç‡ | {metrics.recall:.2f}% |")
        lines.append(f"| F1åˆ†æ•° | {metrics.f1_score:.2f} |")
        
        # Confusion matrix
        lines.append("\n### æ··æ·†çŸ©é˜µ\n")
        lines.append("| | é¢„æµ‹ä¸ºè¿è§„ | é¢„æµ‹ä¸ºæ­£å¸¸ |")
        lines.append("|---|---|---|")
        lines.append(f"| **å®é™…è¿è§„** | TP: {metrics.true_positive} | FN: {metrics.false_negative} |")
        lines.append(f"| **å®é™…æ­£å¸¸** | FP: {metrics.false_positive} | TN: {metrics.true_negative} |")
        
        # Errors
        if metrics.error_types:
            lines.append("\n### é”™è¯¯ç»Ÿè®¡\n")
            lines.append("| é”™è¯¯ç±»å‹ | æ•°é‡ |")
            lines.append("|----------|------|")
            for error_type, count in metrics.error_types.items():
                lines.append(f"| {error_type} | {count} |")
        
        return "\n".join(lines)
    
    def _generate_summary(self, result: BenchmarkResult) -> str:
        """Generate summary section."""
        lines = []
        
        if result.text_metrics:
            tm = result.text_metrics
            lines.append(f"**æ–‡æœ¬å®¡æ ¸:**")
            lines.append(f"- å¤„ç† {tm.total_requests} ä¸ªè¯·æ±‚ï¼ŒæˆåŠŸç‡ {tm.success_rate:.1f}%")
            lines.append(f"- å¹³å‡å“åº”æ—¶é—´: {tm.avg_response_time*1000:.0f}msï¼ŒP99: {tm.p99_response_time*1000:.0f}ms")
            lines.append(f"- å‡†ç¡®ç‡: {tm.accuracy:.1f}%ï¼Œå¬å›ç‡: {tm.recall:.1f}%ï¼ŒF1: {tm.f1_score:.1f}")
            lines.append("")
        
        if result.image_metrics:
            im = result.image_metrics
            lines.append(f"**å›¾ç‰‡å®¡æ ¸:**")
            lines.append(f"- å¤„ç† {im.total_requests} ä¸ªè¯·æ±‚ï¼ŒæˆåŠŸç‡ {im.success_rate:.1f}%")
            lines.append(f"- å¹³å‡å“åº”æ—¶é—´: {im.avg_response_time*1000:.0f}msï¼ŒP99: {im.p99_response_time*1000:.0f}ms")
            lines.append(f"- å‡†ç¡®ç‡: {im.accuracy:.1f}%ï¼Œå¬å›ç‡: {im.recall:.1f}%ï¼ŒF1: {im.f1_score:.1f}")
        
        return "\n".join(lines)
    
    def generate_json(
        self,
        result: BenchmarkResult,
        filename: Optional[str] = None,
    ) -> str:
        """
        Generate JSON benchmark results.
        
        Args:
            result: Benchmark result to export
            filename: Output filename (optional)
            
        Returns:
            Path to generated file
        """
        file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if not filename:
            filename = f"benchmark_results_{result.provider}_{file_timestamp}.json"
        
        output_path = self.output_dir / filename
        
        # Get machine info
        machine_info = get_machine_info()
        
        data = {
            "provider": result.provider,
            "generated_at": datetime.now().isoformat(),
            "test_environment": {
                "region": machine_info["region"],
                "availability_zone": machine_info["availability_zone"],
                "instance_id": machine_info["instance_id"],
                "hostname": machine_info["hostname"],
                "ip_address": machine_info["ip_address"],
                "platform": machine_info["platform"],
            },
            "text_metrics": result.text_metrics.to_dict() if result.text_metrics else None,
            "image_metrics": result.image_metrics.to_dict() if result.image_metrics else None,
        }
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return str(output_path)
    
    def generate_comparison_report(
        self,
        results: Dict[str, BenchmarkResult],
        filename: Optional[str] = None,
    ) -> str:
        """
        Generate a comparison report for multiple providers.
        
        Args:
            results: Dictionary mapping provider name to results
            filename: Output filename (optional)
            
        Returns:
            Path to generated file
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        file_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if not filename:
            filename = f"benchmark_comparison_{file_timestamp}.md"
        
        output_path = self.output_dir / filename
        
        lines = []
        lines.append("# å†…å®¹å®¡æ ¸ä¾›åº”å•†å¯¹æ¯”æŠ¥å‘Š")
        lines.append(f"\n**ç”Ÿæˆæ—¶é—´:** {timestamp}")
        lines.append(f"**å¯¹æ¯”ä¾›åº”å•†:** {', '.join(results.keys())}")
        lines.append("\n---\n")
        
        # Text comparison
        text_results = {
            k: v.text_metrics for k, v in results.items() 
            if v.text_metrics
        }
        if text_results:
            lines.append(self._format_comparison_table(
                "æ–‡æœ¬å®¡æ ¸å¯¹æ¯”",
                text_results,
            ))
        
        # Image comparison
        image_results = {
            k: v.image_metrics for k, v in results.items()
            if v.image_metrics
        }
        if image_results:
            lines.append(self._format_comparison_table(
                "å›¾ç‰‡å®¡æ ¸å¯¹æ¯”",
                image_results,
            ))
        
        # Recommendations
        lines.append("\n## å»ºè®®\n")
        lines.append(self._generate_recommendations(results))
        
        content = "\n".join(lines)
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
        
        return str(output_path)
    
    def _format_comparison_table(
        self,
        title: str,
        metrics_dict: Dict[str, BenchmarkMetrics],
    ) -> str:
        """Format a comparison table for multiple providers."""
        lines = []
        lines.append(f"\n## {title}\n")
        
        providers = list(metrics_dict.keys())
        
        # Performance comparison
        lines.append("### æ€§èƒ½å¯¹æ¯”\n")
        header = "| æŒ‡æ ‡ |" + "|".join(f" {p} " for p in providers) + "|"
        separator = "|------|" + "|".join("------" for _ in providers) + "|"
        lines.append(header)
        lines.append(separator)
        
        rows = [
            ("å¹³å‡å“åº”(ms)", lambda m: f"{m.avg_response_time*1000:.0f}"),
            ("P99å“åº”(ms)", lambda m: f"{m.p99_response_time*1000:.0f}"),
            ("æˆåŠŸç‡(%)", lambda m: f"{m.success_rate:.1f}"),
            ("QPS", lambda m: f"{m.qps:.1f}"),
        ]
        
        for label, getter in rows:
            row = f"| {label} |"
            for p in providers:
                row += f" {getter(metrics_dict[p])} |"
            lines.append(row)
        
        # Accuracy comparison
        lines.append("\n### å‡†ç¡®æ€§å¯¹æ¯”\n")
        lines.append(header)
        lines.append(separator)
        
        rows = [
            ("å‡†ç¡®ç‡(%)", lambda m: f"{m.accuracy:.1f}"),
            ("ç²¾ç¡®ç‡(%)", lambda m: f"{m.precision:.1f}"),
            ("å¬å›ç‡(%)", lambda m: f"{m.recall:.1f}"),
            ("F1åˆ†æ•°", lambda m: f"{m.f1_score:.1f}"),
        ]
        
        for label, getter in rows:
            row = f"| {label} |"
            for p in providers:
                row += f" {getter(metrics_dict[p])} |"
            lines.append(row)
        
        return "\n".join(lines)
    
    def _generate_recommendations(
        self, 
        results: Dict[str, BenchmarkResult],
    ) -> str:
        """Generate recommendations based on results."""
        lines = []
        
        # Find best performers
        text_results = {
            k: v.text_metrics for k, v in results.items()
            if v.text_metrics
        }
        
        if text_results:
            # Best response time
            best_speed = min(
                text_results.items(),
                key=lambda x: x[1].avg_response_time,
            )
            lines.append(f"- **å“åº”æœ€å¿«:** {best_speed[0]} (å¹³å‡ {best_speed[1].avg_response_time*1000:.0f}ms)")
            
            # Best accuracy
            best_accuracy = max(
                text_results.items(),
                key=lambda x: x[1].accuracy,
            )
            lines.append(f"- **å‡†ç¡®ç‡æœ€é«˜:** {best_accuracy[0]} ({best_accuracy[1].accuracy:.1f}%)")
            
            # Best recall
            best_recall = max(
                text_results.items(),
                key=lambda x: x[1].recall,
            )
            lines.append(f"- **å¬å›ç‡æœ€é«˜:** {best_recall[0]} ({best_recall[1].recall:.1f}%)")
        
        lines.append("\n### åœºæ™¯æ¨è\n")
        lines.append("| ä½¿ç”¨åœºæ™¯ | æ¨èä¾›åº”å•† | åŸå›  |")
        lines.append("|----------|------------|------|")
        
        if text_results:
            best_speed_name = min(text_results.items(), key=lambda x: x[1].avg_response_time)[0]
            best_recall_name = max(text_results.items(), key=lambda x: x[1].recall)[0]
            best_f1_name = max(text_results.items(), key=lambda x: x[1].f1_score)[0]
            
            lines.append(f"| å®æ—¶å®¡æ ¸ | {best_speed_name} | å“åº”é€Ÿåº¦æœ€å¿« |")
            lines.append(f"| é«˜é£é™©å†…å®¹ | {best_recall_name} | å¬å›ç‡æœ€é«˜ï¼Œæ¼æ£€æœ€å°‘ |")
            lines.append(f"| ç»¼åˆå¹³è¡¡ | {best_f1_name} | F1åˆ†æ•°æœ€ä¼˜ |")
        
        return "\n".join(lines)
    
    def print_summary(self, result: BenchmarkResult) -> None:
        """Print a summary to console."""
        print("\n" + "="*60)
        print(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šæ‘˜è¦: {result.provider}")
        print("="*60)
        
        if result.text_metrics:
            tm = result.text_metrics
            print(f"\nğŸ“ æ–‡æœ¬å®¡æ ¸:")
            print(f"   è¯·æ±‚æ€»æ•°: {tm.total_requests}")
            print(f"   æˆåŠŸç‡: {tm.success_rate:.1f}%")
            print(f"   å¹³å‡å“åº”: {tm.avg_response_time*1000:.0f}ms")
            print(f"   P99å“åº”: {tm.p99_response_time*1000:.0f}ms")
            print(f"   å‡†ç¡®ç‡: {tm.accuracy:.1f}%")
            print(f"   å¬å›ç‡: {tm.recall:.1f}%")
            print(f"   F1åˆ†æ•°: {tm.f1_score:.1f}")
        
        if result.image_metrics:
            im = result.image_metrics
            print(f"\nğŸ–¼ï¸  å›¾ç‰‡å®¡æ ¸:")
            print(f"   è¯·æ±‚æ€»æ•°: {im.total_requests}")
            print(f"   æˆåŠŸç‡: {im.success_rate:.1f}%")
            print(f"   å¹³å‡å“åº”: {im.avg_response_time*1000:.0f}ms")
            print(f"   P99å“åº”: {im.p99_response_time*1000:.0f}ms")
            print(f"   å‡†ç¡®ç‡: {im.accuracy:.1f}%")
            print(f"   å¬å›ç‡: {im.recall:.1f}%")
            print(f"   F1åˆ†æ•°: {im.f1_score:.1f}")
        
        print("\n" + "="*60)
